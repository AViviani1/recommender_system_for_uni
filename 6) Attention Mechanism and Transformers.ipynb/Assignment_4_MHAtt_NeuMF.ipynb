{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yVlwEEkWlXEu"
      },
      "source": [
        "# LAB 4: Attention"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Assignment made by:\n",
        "\n",
        "Alessandro Viviani 843234\n",
        "\n",
        "Francesca Arredondo 820354\n",
        "\n",
        "Fabio Turchetta 898572\n"
      ],
      "metadata": {
        "id": "RS7fI7wlbnTU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Yji9-7jslKRz"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "import random\n",
        "import argparse\n",
        "import numpy as np \n",
        "import pandas as pd \n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as data\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import matplotlib.pyplot as plt \n",
        "import torch.nn.functional as F\n",
        "from torch.nn import TransformerEncoder, TransformerEncoderLayer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "arJu3Re7XCky",
        "outputId": "9e5c218d-817c-4d08-8740-ed97aad6142a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f75501d9930>"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "np.random.seed(55)\n",
        "torch.manual_seed(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "IkcVzJcY5TS_"
      },
      "outputs": [],
      "source": [
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "qcffYFMdxDcI"
      },
      "outputs": [],
      "source": [
        "dataset_origin = {'100k': 'u.data', '1M': 'ratings.dat'}\n",
        "\n",
        "num_sample_data = '100k'\n",
        "DATA_PATH = 'drive/MyDrive/Colab Notebooks/ml-100k/u.data'.format(dataset_origin[num_sample_data]) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XX0MtVLM4E_k",
        "outputId": "7a987785-9b74-4e26-db73-d1d3682f7db8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Class Rating_Dataset"
      ],
      "metadata": {
        "id": "NdGJWM8FsgmQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "hmlijxEoYXDj",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "class Rating_Datset(Dataset):\n",
        "\tdef __init__(self, user_list, item_list, rating_list):\n",
        "\t\tsuper(Rating_Datset, self).__init__()\n",
        "\t\tself.user_list = user_list\n",
        "\t\tself.item_list = item_list\n",
        "\t\tself.rating_list = rating_list\n",
        "\n",
        "\tdef __len__(self):\n",
        "\t\treturn len(self.user_list)\n",
        "\n",
        "\tdef __getitem__(self, idx):\n",
        "\t\tuser = self.user_list[idx]\n",
        "\t\titem = self.item_list[idx]\n",
        "\t\trating = self.rating_list[idx]\n",
        "\t\t\n",
        "\t\treturn (\n",
        "\t\t\ttorch.tensor(user, dtype=torch.long),\n",
        "\t\t\ttorch.tensor(item, dtype=torch.long),\n",
        "\t\t\ttorch.tensor(rating, dtype=torch.float)\n",
        "\t\t\t)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Class NCF_Data"
      ],
      "metadata": {
        "id": "ihc99EIhsnll"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "FxZCDy4tYkRZ",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "class NCF_Data(object):\n",
        "\t\"\"\"\n",
        "\tConstruct Dataset for NCF\n",
        "\t\"\"\"\n",
        "\tdef __init__(self, args, ratings):\n",
        "\t\tself.ratings = ratings\n",
        "\t\tself.num_ng = args.num_ng\n",
        "\t\tself.num_ng_test = args.num_ng_test\n",
        "\t\tself.batch_size = args.batch_size\n",
        "\n",
        "\t\tself.preprocess_ratings = self._reindex(self.ratings)\n",
        "\n",
        "\t\tself.user_pool = set(self.ratings['user_id'].unique())\n",
        "\t\tself.item_pool = set(self.ratings['item_id'].unique())\n",
        "\n",
        "\t\tself.train_ratings, self.test_ratings = self._leave_one_out(self.preprocess_ratings)\n",
        "\t\tself.negatives = self._negative_sampling(self.preprocess_ratings)\n",
        "\n",
        "\t\n",
        "\tdef _reindex(self, ratings):\n",
        "\t\t\"\"\"\n",
        "\t\tProcess dataset to reindex userID and itemID, also set rating as binary feedback\n",
        "\t\t\"\"\"\n",
        "\t\tuser_list = list(ratings['user_id'].drop_duplicates())\n",
        "\t\tuser2id = {w: i for i, w in enumerate(user_list)}\n",
        "\n",
        "\t\titem_list = list(ratings['item_id'].drop_duplicates())\n",
        "\t\titem2id = {w: i for i, w in enumerate(item_list)}\n",
        "\n",
        "\t\tratings['user_id'] = ratings['user_id'].apply(lambda x: user2id[x])\n",
        "\t\tratings['item_id'] = ratings['item_id'].apply(lambda x: item2id[x])\n",
        "\t\tratings['rating'] = ratings['rating'].apply(lambda x: float(x > 0))\n",
        "\t\treturn ratings\n",
        "\n",
        "\tdef _leave_one_out(self, ratings):\n",
        "\t\t\"\"\"\n",
        "\t\tleave-one-out evaluation protocol in paper https://www.comp.nus.edu.sg/~xiangnan/papers/ncf.pdf\n",
        "\t\t\"\"\"\n",
        "\t\tratings['rank_latest'] = ratings.groupby(['user_id'])['timestamp'].rank(method='first', ascending=False)\n",
        "\t\ttest = ratings.loc[ratings['rank_latest'] == 1]\n",
        "\t\ttrain = ratings.loc[ratings['rank_latest'] > 1]\n",
        "\t\tassert train['user_id'].nunique()==test['user_id'].nunique(), 'Not Match Train User with Test User'\n",
        "\t\treturn train[['user_id', 'item_id', 'rating']], test[['user_id', 'item_id', 'rating']]\n",
        "\n",
        "\tdef _negative_sampling(self, ratings):\n",
        "\t\tinteract_status = (\n",
        "\t\t\tratings.groupby('user_id')['item_id']\n",
        "\t\t\t.apply(set)\n",
        "\t\t\t.reset_index()\n",
        "\t\t\t.rename(columns={'item_id': 'interacted_items'}))\n",
        "\t\tinteract_status['negative_items'] = interact_status['interacted_items'].apply(lambda x: self.item_pool - x)\n",
        "\t\tinteract_status['negative_samples'] = interact_status['negative_items'].apply(lambda x: random.sample(x, self.num_ng_test))\n",
        "\t\treturn interact_status[['user_id', 'negative_items', 'negative_samples']]\n",
        "\n",
        "\tdef get_train_instance(self):\n",
        "\t\tusers, items, ratings = [], [], []\n",
        "\t\ttrain_ratings = pd.merge(self.train_ratings, self.negatives[['user_id', 'negative_items']], on='user_id')\n",
        "\t\ttrain_ratings['negatives'] = train_ratings['negative_items'].apply(lambda x: random.sample(x, self.num_ng))\n",
        "\t\tfor row in train_ratings.itertuples():\n",
        "\t\t\tusers.append(int(row.user_id))\n",
        "\t\t\titems.append(int(row.item_id))\n",
        "\t\t\tratings.append(float(row.rating))\n",
        "\t\t\tfor i in range(self.num_ng):\n",
        "\t\t\t\tusers.append(int(row.user_id))\n",
        "\t\t\t\titems.append(int(row.negatives[i]))\n",
        "\t\t\t\tratings.append(float(0))  # negative samples get 0 rating\n",
        "\t\tdataset = Rating_Datset(\n",
        "\t\t\tuser_list=users,\n",
        "\t\t\titem_list=items,\n",
        "\t\t\trating_list=ratings)\n",
        "\t\treturn DataLoader(dataset, batch_size=self.batch_size, shuffle=True, num_workers=4)\n",
        "\n",
        "\tdef get_test_instance(self):\n",
        "\t\tusers, items, ratings = [], [], []\n",
        "\t\ttest_ratings = pd.merge(self.test_ratings, self.negatives[['user_id', 'negative_samples']], on='user_id')\n",
        "\t\tfor row in test_ratings.itertuples():\n",
        "\t\t\tusers.append(int(row.user_id))\n",
        "\t\t\titems.append(int(row.item_id))\n",
        "\t\t\tratings.append(float(row.rating))\n",
        "\t\t\tfor i in getattr(row, 'negative_samples'):\n",
        "\t\t\t\tusers.append(int(row.user_id))\n",
        "\t\t\t\titems.append(int(i))\n",
        "\t\t\t\tratings.append(float(0))\n",
        "\t\tdataset = Rating_Datset(\n",
        "\t\t\tuser_list=users,\n",
        "\t\t\titem_list=items,\n",
        "\t\t\trating_list=ratings)\n",
        "\t\treturn DataLoader(dataset, batch_size=self.num_ng_test+1, shuffle=False, num_workers=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Metrics"
      ],
      "metadata": {
        "id": "7w1aoplWs0rK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "uTaIX7H52i-a",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "def hit(ng_item, pred_items):\n",
        "\tif ng_item in pred_items:\n",
        "\t\treturn 1\n",
        "\treturn 0\n",
        "\n",
        "\n",
        "def ndcg(ng_item, pred_items):\n",
        "\tif ng_item in pred_items:\n",
        "\t\tindex = pred_items.index(ng_item)\n",
        "\t\treturn np.reciprocal(np.log2(index+2))\n",
        "\treturn 0\n",
        "\n",
        "\n",
        "def metrics(model, test_loader, top_k, device):\n",
        "\tHR, NDCG = [], []\n",
        "\n",
        "\tfor user, item, label in test_loader:\n",
        "\t\tuser = user.to(device)\n",
        "\t\titem = item.to(device)\n",
        "\n",
        "\t\tpredictions = model(user, item)\n",
        "\t\t_, indices = torch.topk(predictions, top_k)\n",
        "\t\trecommends = torch.take(\n",
        "\t\t\t\titem, indices).cpu().numpy().tolist()\n",
        "\n",
        "\t\tng_item = item[0].item() # leave one-out evaluation has only one item per user\n",
        "\t\tHR.append(hit(ng_item, recommends))\n",
        "\t\tNDCG.append(ndcg(ng_item, recommends))\n",
        "\n",
        "\treturn np.mean(HR), np.mean(NDCG)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Class MHSelfAttention"
      ],
      "metadata": {
        "id": "ECTGDHXftdeN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "emb=32\n",
        "heads=8"
      ],
      "metadata": {
        "id": "QG0nGvjMwHK5"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "NrvYJ5tPYI7L"
      },
      "outputs": [],
      "source": [
        "class MHSelfAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Multi-head self attention.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, emb, heads=8):\n",
        "        \"\"\"\n",
        "        :param emb:\n",
        "        :param heads:\n",
        "        :param mask:\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        assert emb % heads == 0, f'Embedding dimension ({emb}) should be divisible by nr. of heads ({heads})'\n",
        "\n",
        "        self.emb = emb\n",
        "        self.heads = heads\n",
        "\n",
        "        #s = emb // heads\n",
        "        # - We will break the embedding into `heads` chunks and feed each to a different attention head\n",
        "\n",
        "        self.tokeys    = nn.Linear(emb, emb, bias=False)\n",
        "        self.toqueries = nn.Linear(emb, emb, bias=False)\n",
        "        self.tovalues  = nn.Linear(emb, emb, bias=False)\n",
        "\n",
        "        self.unifyheads = nn.Linear(emb, emb)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \n",
        "        #b=256\n",
        "        #e=32\n",
        "        b, e = x.size()\n",
        "        h = self.heads\n",
        "        #h=8\n",
        "        assert e == self.emb, f'Input embedding dim ({e}) should match layer embedding dim ({self.emb})'\n",
        "\n",
        "        s = e // h \n",
        "\n",
        "        # We first compute the k/q/v's on the whole embedding vectors, and then split into the different heads.\n",
        "\n",
        "        keys    = self.tokeys(x)\n",
        "        queries = self.toqueries(x)\n",
        "        values  = self.tovalues(x)\n",
        "\n",
        "       # Split into the different heads.\n",
        "\n",
        "        keys    = keys.view(b, 1, h, s)    \n",
        "        queries = queries.view(b, 1, h, s)\n",
        "        values  = values.view(b, 1, h, s)\n",
        "\n",
        "        # Compute scaled dot-product self-attention\n",
        "\n",
        "        # Fold heads into the batch dimension\n",
        "        # When you call contiguous(), it actually makes a copy of the tensor \n",
        "        # such that the order of its elements in memory is the same as if it had been created from scratch with the same data.\n",
        "        keys = keys.transpose(1, 2).contiguous().view(b*h, 1, s)   \n",
        "        queries = queries.transpose(1, 2).contiguous().view(b*h, 1, s)\n",
        "        values = values.transpose(1, 2).contiguous().view(b*h, 1, s)\n",
        "\n",
        "        queries = queries / (e ** (1/4))\n",
        "        keys    = keys / (e ** (1/4))\n",
        "        # Instead of dividing the dot products by sqrt(e), we scale the keys and values.\n",
        "        # This should be more memory efficient\n",
        "\n",
        "        # Get dot product of queries and keys, and scale.\n",
        "\n",
        "        dot = torch.bmm(queries, keys.transpose(1, 2))\n",
        "\n",
        "        assert dot.size() == (b*h, 1, 1)\n",
        "\n",
        "        dot = F.softmax(dot, dim=2) # Dot now has row-wise self-attention probabilities\n",
        "\n",
        "        # apply the self attention to the values\n",
        "        out = torch.bmm(dot, values).view(b, h, 1, s)\n",
        "\n",
        "        # swap h, t back, unify heads\n",
        "        out = out.transpose(1, 2).contiguous().view(b, 1, s * h)\n",
        "\n",
        "        return self.unifyheads(out)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Class NeuMF"
      ],
      "metadata": {
        "id": "8cF9aqO_tQKK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "2NYmT3EwSHFU"
      },
      "outputs": [],
      "source": [
        "# Define Neural Matrix Factorization class\n",
        "class NeuMF(nn.Module):\n",
        "    def __init__(self, args, num_users, num_items):\n",
        "        super(NeuMF, self).__init__()\n",
        "        self.num_users = num_users\n",
        "        self.num_items = num_items\n",
        "        self.factor_num = args.factor_num\n",
        "        self.layers = args.layers\n",
        "\n",
        "        self.gmf_user_embedding = nn.Embedding(self.num_users, self.factor_num)\n",
        "        self.gmf_item_embedding = nn.Embedding(self.num_items, self.factor_num)\n",
        "        self.mlp_user_embedding = nn.Embedding(self.num_users, self.factor_num)\n",
        "        self.mlp_item_embedding = nn.Embedding(self.num_items, self.factor_num)\n",
        "        \n",
        "        mlp_layer_sizes = self.layers\n",
        "        self.mlp_layers = nn.ModuleList()\n",
        "        for idx, (in_size, out_size) in enumerate(zip(mlp_layer_sizes[:-1], mlp_layer_sizes[1:])):\n",
        "            self.mlp_layers.append(nn.Linear(in_size, out_size))\n",
        "\n",
        "        self.output_layer = nn.Linear(in_features=mlp_layer_sizes[-1] + self.factor_num, out_features=1)\n",
        "        self.logistic = torch.sigmoid\n",
        "\n",
        "    def forward(self, user_indices, item_indices):\n",
        "        # Matrix factorization branch\n",
        "        gmf_user_embedding = self.gmf_user_embedding(user_indices)\n",
        "        gmf_item_embedding = self.gmf_item_embedding(item_indices)\n",
        "        gmf_output = torch.mul(gmf_user_embedding, gmf_item_embedding) # GMF layer\n",
        "        \n",
        "        # Multilayer perceptron branch\n",
        "        mlp_user_embedding = self.mlp_user_embedding(user_indices)\n",
        "        mlp_item_embedding = self.mlp_item_embedding(item_indices)\n",
        "        mlp_user_attention = self_attention(mlp_user_embedding)\n",
        "        mlp_item_attention = self_attention(mlp_item_embedding)\n",
        "        mlp_user_attention = mlp_user_attention.squeeze(dim=1)\n",
        "        mlp_item_attention = mlp_item_attention.squeeze(dim=1)\n",
        "        mlp_output = torch.cat([mlp_user_attention,mlp_item_attention], dim=-1) \n",
        "        for idx, _ in enumerate(range(len(self.mlp_layers))):\n",
        "            mlp_output = self.mlp_layers[idx](mlp_output)\n",
        "            mlp_output = nn.ReLU()(mlp_output)\n",
        "        \n",
        "        # Concatenate GMF and MLP outputs and pass through final layer\n",
        "        concatenated = torch.cat([gmf_output, mlp_output], dim=1)\n",
        "        logits = self.output_layer(concatenated)\n",
        "        rating = self.logistic(logits)\n",
        "        return rating.squeeze()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TzFNcyild5y8",
        "outputId": "47a62404-ac2d-400a-fccd-eddf302d2515"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "_StoreAction(option_strings=['--out'], dest='out', nargs=None, const=None, default=True, type=None, choices=None, required=False, help='save model or not', metavar=None)"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ],
      "source": [
        "#collapse-hide\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument(\"--seed\", \n",
        "\ttype=int, \n",
        "\tdefault=51, \n",
        "\thelp=\"Seed\")\n",
        "parser.add_argument(\"--lr\", \n",
        "\ttype=float, \n",
        "\tdefault=0.001, \n",
        "\thelp=\"learning rate\")\n",
        "parser.add_argument(\"--dropout\", \n",
        "\ttype=float,\n",
        "\tdefault=0.2,  \n",
        "\thelp=\"dropout rate\")\n",
        "parser.add_argument(\"--batch_size\", \n",
        "\ttype=int, \n",
        "\tdefault=256, \n",
        "\thelp=\"batch size for training\")\n",
        "parser.add_argument(\"--epochs\", \n",
        "\ttype=int,\n",
        "\tdefault=10,  \n",
        "\thelp=\"training epoches\")\n",
        "parser.add_argument(\"--top_k\", \n",
        "\ttype=int, \n",
        "\tdefault=10, \n",
        "\thelp=\"compute metrics@top_k\")\n",
        "parser.add_argument(\"--factor_num\", \n",
        "\ttype=int,\n",
        "\tdefault=32, \n",
        "\thelp=\"predictive factors numbers in the model\")\n",
        "parser.add_argument(\"--layers\",\n",
        "    nargs='+', \n",
        "    default=[64,32,16,8],\n",
        "    help=\"MLP layers. Note that the first layer is the concatenation of user \\\n",
        "    and item embeddings. So layers[0]/2 is the embedding size.\")\n",
        "parser.add_argument(\"--num_ng\", \n",
        "\ttype=int,\n",
        "\tdefault=4, \n",
        "\thelp=\"Number of negative samples for training set\")\n",
        "parser.add_argument(\"--num_ng_test\", \n",
        "\ttype=int,\n",
        "\tdefault=100, \n",
        "\thelp=\"Number of negative samples for test set\")\n",
        "parser.add_argument(\"--out\", \n",
        "\tdefault=True,\n",
        "\thelp=\"save model or not\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Main"
      ],
      "metadata": {
        "id": "seLI6-BS_-wY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "coXQgQWYd51r",
        "outputId": "2be9404b-80e8-4540-e3c7-ecdbeeb83e27"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-7-fa01be874ac8>:53: DeprecationWarning: Sampling from a set deprecated\n",
            "since Python 3.9 and will be removed in a subsequent version.\n",
            "  interact_status['negative_samples'] = interact_status['negative_items'].apply(lambda x: random.sample(x, self.num_ng_test))\n",
            "<ipython-input-7-fa01be874ac8>:59: DeprecationWarning: Sampling from a set deprecated\n",
            "since Python 3.9 and will be removed in a subsequent version.\n",
            "  train_ratings['negatives'] = train_ratings['negative_items'].apply(lambda x: random.sample(x, self.num_ng))\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        }
      ],
      "source": [
        " # set device and parameters\n",
        "args = parser.parse_args(\"\")\n",
        "device = torch.device(\"cpu\")\n",
        "\n",
        "# load data\n",
        "ml_100k = pd.read_csv(\n",
        "\tDATA_PATH, \n",
        "\tsep=\"\\t\", \n",
        "\tnames = ['user_id', 'item_id', 'rating', 'timestamp'], \n",
        "\tengine='python')\n",
        "\n",
        "# set the num_users, items\n",
        "num_users = ml_100k['user_id'].nunique()+1\n",
        "num_items = ml_100k['item_id'].nunique()+1\n",
        "\n",
        "# construct the train and test datasets\n",
        "data = NCF_Data(args, ml_100k)\n",
        "train_loader = data.get_train_instance()\n",
        "test_loader = data.get_test_instance()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "self_attention = MHSelfAttention(emb, heads)"
      ],
      "metadata": {
        "id": "qoRBKI5OxYFH"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# set model and loss, optimizer\n",
        "model = NeuMF(args, num_users, num_items)\n",
        "model = model.to(device)\n",
        "loss_function = nn.BCELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=args.lr)"
      ],
      "metadata": {
        "id": "5H8ua7Y-xZy0"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train, evaluation\n",
        "best_hr = 0\n",
        "for epoch in range(1, args.epochs+1):\n",
        "\tmodel.train() # Enable dropout (if have).\n",
        "\tstart_time = time.time()\n",
        "\n",
        "\tfor user, item, label in train_loader:\n",
        "\t\tuser = user.to(device)\n",
        "\t\titem = item.to(device)\n",
        "\t\tlabel = label.to(device)\n",
        "\n",
        "\t\toptimizer.zero_grad()\n",
        "\t\tprediction = model(user, item)\n",
        "\t\tloss = loss_function(prediction, label)\n",
        "\t\tloss.backward()\n",
        "\t\toptimizer.step()\n",
        "\t\t#writer.add_scalar('loss/Train_loss', loss.item(), epoch)\n",
        "\n",
        "\tmodel.eval()\n",
        "\tHR, NDCG = metrics(model, test_loader, args.top_k, device)\n",
        "\t#writer.add_scalar('Perfomance/HR@10', HR, epoch)\n",
        "\t#writer.add_scalar('Perfomance/NDCG@10', NDCG, epoch)\n",
        "\n",
        "\telapsed_time = time.time() - start_time\n",
        "\tprint(\"Epoch {:03d}\".format(epoch) + \" time to train: \" + \n",
        "\t\t\ttime.strftime(\"%H: %M: %S\", time.gmtime(elapsed_time)))\n",
        "\tprint(\"HR: {:.3f}\\tNDCG: {:.3f}\".format(np.mean(HR), np.mean(NDCG)))\n",
        "\n",
        "\tif HR > best_hr:\n",
        "\t\tbest_hr, best_ndcg, best_epoch = HR, NDCG, epoch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hz1CtehkuCWM",
        "outputId": "7231a87c-2fff-4610-831d-aaf5f96d61c0"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001 time to train: 00: 00: 34\n",
            "HR: 0.403\tNDCG: 0.218\n",
            "Epoch 002 time to train: 00: 00: 35\n",
            "HR: 0.405\tNDCG: 0.226\n",
            "Epoch 003 time to train: 00: 00: 33\n",
            "HR: 0.405\tNDCG: 0.224\n",
            "Epoch 004 time to train: 00: 00: 33\n",
            "HR: 0.400\tNDCG: 0.220\n",
            "Epoch 005 time to train: 00: 00: 35\n",
            "HR: 0.400\tNDCG: 0.222\n",
            "Epoch 006 time to train: 00: 00: 33\n",
            "HR: 0.411\tNDCG: 0.223\n",
            "Epoch 007 time to train: 00: 00: 33\n",
            "HR: 0.405\tNDCG: 0.222\n",
            "Epoch 008 time to train: 00: 00: 35\n",
            "HR: 0.431\tNDCG: 0.233\n",
            "Epoch 009 time to train: 00: 00: 33\n",
            "HR: 0.433\tNDCG: 0.236\n",
            "Epoch 010 time to train: 00: 00: 33\n",
            "HR: 0.432\tNDCG: 0.239\n"
          ]
        }
      ]
    }
  ]
}